This project is carried out as part of the engineering cycle training (second year) of the Polytechnic School of the Université Côte d'Azur.
***
# Automatic detection of sign language gestures

## Présentation
This project was carried out with the Python language.

### Purpose:
* Detect letters and words of sign language in real time.
* Use machine and deep learning techniques such as boosting, neural networks (multilayer perceptron), deep neural network (convolutionnal neural network) and transfer learning.

Example of result:

We observe that the CNN is more efficient than the MLP, but the CNN is slightly slower than the MLP.
For the MLP, the execution time for 100 images is 34.51 seconds and the accuracy is 0.8.
For the CNN, the execution time for 100 images is 45.68 seconds and the accuracy is 1.0.
We observe that the MLP from the VGG is more efficient than the MLP and equivalent to the CNN (for the accuracy). And this algorithm is more faster than the two other algorithms. Indeed, the scores are:
The execution time for 600 images is 0.21 seconds and the accuracy is 1.0.
Finally, transfer learning speeded up training and improved the performance of our deep learning model.

![alt text](https://github.com/JulienChoukroun/Face-Detection-Orientation-Recognition/blob/main/Images/videoTram6-1.png "Tram6-1")
